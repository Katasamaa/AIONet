import os
from datasets import load_dataset, list_datasets
from huggingface_hub import HfApi, DatasetInfo
import pandas as pd

class HuggingFaceIntegration:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Hugging Face –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ü–æ–∏—Å–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
    - –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (—Ä–∞–∑–º–µ—Ä, –æ–ø–∏—Å–∞–Ω–∏–µ, –∑–∞–¥–∞—á–∏)
    - –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    - –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ pandas DataFrame
    """
    
    def __init__(self, token=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Hugging Face
        
        Args:
            token: HF —Ç–æ–∫–µ–Ω (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ HUGGINGFACE_TOKEN)
        """
        self.token = token or os.getenv("HUGGINGFACE_TOKEN")
        
        # –¢–æ–∫–µ–Ω –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω –¥–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if not self.token:
            print("‚ö†Ô∏è HUGGINGFACE_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω. –†–∞–±–æ—Ç–∞ —Ç–æ–ª—å–∫–æ —Å –ø—É–±–ª–∏—á–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏.")
        else:
            print("‚úÖ Hugging Face API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        self.api = HfApi(token=self.token)
        self.cache = {}  # –ö—ç—à –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    
    def search_datasets(self, query, task_type=None, limit=10):
        """
        –ü–æ–∏—Å–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
        
        Args:
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            task_type (str): –¢–∏–ø –∑–∞–¥–∞—á–∏ (text-classification, image-classification, etc.)
            limit (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
            
        Example:
            >>> hf.search_datasets("sentiment", task_type="text-classification")
            [
                {
                    "id": "imdb",
                    "description": "Large Movie Review Dataset",
                    "downloads": 125000,
                    "task": "text-classification"
                },
                ...
            ]
        """
        try:
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏
            task_filter = task_type if task_type else None
            
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API
            datasets = self.api.list_datasets(
                search=query,
                task_categories=task_filter,
                sort="downloads",
                direction=-1,
                limit=limit
            )
            
            results = []
            for dataset in datasets:
                results.append({
                    "id": dataset.id,
                    "author": dataset.author,
                    "downloads": getattr(dataset, 'downloads', 0),
                    "likes": getattr(dataset, 'likes', 0),
                    "tags": getattr(dataset, 'tags', [])
                })
            
            return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {e}")
            return []
    
    def get_dataset_info(self, dataset_id):
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
        
        Args:
            dataset_id (str): ID –¥–∞—Ç–∞—Å–µ—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "imdb")
            
        Returns:
            dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if dataset_id in self.cache:
            return self.cache[dataset_id]
        
        try:
            info = self.api.dataset_info(dataset_id)
            
            result = {
                "id": dataset_id,
                "description": getattr(info, 'description', '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è'),
                "citation": getattr(info, 'citation', ''),
                "homepage": getattr(info, 'homepage', ''),
                "license": getattr(info, 'license', 'unknown'),
                "features": str(getattr(info, 'features', {})),
                "splits": list(getattr(info, 'splits', {}).keys()),
                "download_size": getattr(info, 'download_size', 0),
                "dataset_size": getattr(info, 'dataset_size', 0),
            }
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à
            self.cache[dataset_id] = result
            
            return result
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
            return {
                "id": dataset_id,
                "error": str(e)
            }
    
    def load_dataset_as_dataframe(self, dataset_id, split="train", max_rows=1000):
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ pandas DataFrame
        
        Args:
            dataset_id (str): ID –¥–∞—Ç–∞—Å–µ—Ç–∞
            split (str): –†–∞–∑–±–∏–µ–Ω–∏–µ (train/test/validation)
            max_rows (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ (–¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
            
        Returns:
            pd.DataFrame: –î–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ DataFrame
            
        Example:
            >>> df = hf.load_dataset_as_dataframe("imdb", split="train", max_rows=100)
            >>> df.head()
        """
        try:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ {dataset_id} ({split})...")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            dataset = load_dataset(
                dataset_id,
                split=split,
                streaming=False,  # –ü–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–¥–ª—è –º–∞–ª—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)
                token=self.token
            )
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
            if len(dataset) > max_rows:
                dataset = dataset.select(range(max_rows))
                print(f"‚ö†Ô∏è –î–∞—Ç–∞—Å–µ—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–æ {max_rows} —Å—Ç—Ä–æ–∫")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ DataFrame
            df = pd.DataFrame(dataset)
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
            
            return df
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return None
    
    def get_popular_datasets(self, task_category=None, limit=20):
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        
        Args:
            task_category (str): –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–¥–∞—á–∏
            limit (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        """
        try:
            datasets = self.api.list_datasets(
                task_categories=task_category,
                sort="downloads",
                direction=-1,
                limit=limit
            )
            
            results = []
            for ds in datasets:
                results.append({
                    "id": ds.id,
                    "downloads": getattr(ds, 'downloads', 0),
                    "tags": getattr(ds, 'tags', [])
                })
            
            return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {e}")
            return []
    
    def recommend_dataset(self, task_description, task_type="tabular"):
        """
        –ù–û–í–´–ô: –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–∏—Å–∞–Ω–∏—è –∑–∞–¥–∞—á–∏
        
        Args:
            task_description (str): –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            task_type (str): –¢–∏–ø –∑–∞–¥–∞—á–∏ (tabular/text/image/audio)
            
        Returns:
            str: ID —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        """
        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ HF
        task_mapping = {
            "tabular": "tabular-classification",
            "text": "text-classification",
            "image": "image-classification",
            "audio": "audio-classification"
        }
        
        hf_task = task_mapping.get(task_type, None)
        
        # –ü–æ–∏—Å–∫ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é
        results = self.search_datasets(
            query=task_description,
            task_type=hf_task,
            limit=5
        )
        
        if results:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π
            return results[0]["id"]
        
        return None


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø—Ä—è–º–æ–º –∑–∞–ø—É—Å–∫–µ)
if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Hugging Face Integration\n")
    
    try:
        hf = HuggingFaceIntegration()
        
        # –¢–µ—Å—Ç 1: –ü–æ–∏—Å–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        print("üìù –¢–µ—Å—Ç 1: –ü–æ–∏—Å–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø—Ä–æ sentiment")
        results = hf.search_datasets("sentiment", limit=3)
        for ds in results:
            print(f"  - {ds['id']} (downloads: {ds['downloads']})")
        
        print("\n" + "="*50 + "\n")
        
        # –¢–µ—Å—Ç 2: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
        print("üìù –¢–µ—Å—Ç 2: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ 'imdb'")
        info = hf.get_dataset_info("imdb")
        print(f"  –û–ø–∏—Å–∞–Ω–∏–µ: {info.get('description', 'N/A')[:100]}...")
        print(f"  –õ–∏—Ü–µ–Ω–∑–∏—è: {info.get('license', 'N/A')}")
        print(f"  Splits: {info.get('splits', [])}")
        
        print("\n" + "="*50 + "\n")
        
        # –¢–µ—Å—Ç 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("üìù –¢–µ—Å—Ç 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ 'imdb' (100 —Å—Ç—Ä–æ–∫)")
        df = hf.load_dataset_as_dataframe("imdb", split="train", max_rows=100)
        if df is not None:
            print(f"  Shape: {df.shape}")
            print(f"  Columns: {list(df.columns)}")
            print(f"\n–ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏:")
            print(df.head(3))
        
        print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã Hugging Face –ø—Ä–æ–π–¥–µ–Ω—ã!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("\nüí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ datasets:")
        print("   pip install datasets huggingface-hub")